{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8702731-9875-412f-b08e-84a629b179c5",
   "metadata": {},
   "source": [
    "Name: Tan Yee Yang\n",
    "<br>\n",
    "Student ID: 20414203\n",
    "<br>\n",
    "T must be > 89% \n",
    "<br>\n",
    "use the evaluate algorithm provided and not own algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e707edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob # use to extract image and mask part from respective folder\n",
    "from tqdm import tqdm # for progress bar\n",
    "import imageio # to read scientific picture as it support a lot of image format\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate # for Data Argumentation\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95760dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data\n",
    "def load_data(path, test_retina_folder_names, train_retina_folder_names, masked_retina_folder_name):\n",
    "\n",
    "    ## Test image ##\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "\n",
    "    for folder_name in test_retina_folder_names:\n",
    "        retina_images = sorted(glob(os.path.join(path, folder_name,'*.tif')))\n",
    "        test_x.extend(retina_images)   # extend the existing list with new retina images\n",
    "    \n",
    "    # test_masked_retina_images = sorted(glob(os.path.join(path, masked_retina_folder_name,'*.tif')))\n",
    "    # test_y = test_masked_retina_images\n",
    "\n",
    "    \n",
    "    ## Train image ##\n",
    "    train_x = [] # for retina images\n",
    "    train_y = [] # for masked retina images\n",
    "    \n",
    "    for folder_name in train_retina_folder_names:\n",
    "        retina_images = sorted(glob(os.path.join(path, folder_name,'*.tif')))\n",
    "        train_x.extend(retina_images)   # extend the existing list with new retina images\n",
    "    \n",
    "    masked_retina_images = sorted(glob(os.path.join(path, masked_retina_folder_name,'*.tif')))\n",
    "    \n",
    "    # Extract base filenames without extension for matching\n",
    "    base_filenames_train_x = [os.path.splitext(os.path.basename(x))[0] for x in train_x]\n",
    "    base_filenames_test_x = [os.path.splitext(os.path.basename(x))[0] for x in test_x]\n",
    "    \n",
    "    # Filter masked_retina_images based on matching base filenames\n",
    "    for masked_image in masked_retina_images:\n",
    "        base_filename_masked = os.path.splitext(os.path.basename(masked_image))[0]\n",
    "        if base_filename_masked in base_filenames_train_x:\n",
    "            train_y.append(masked_image)\n",
    "        if base_filename_masked in base_filenames_test_x:\n",
    "            test_y.append(masked_image)\n",
    "        \n",
    "    # train_masked_retina_images = sorted(glob(os.path.join(path, masked_retina_folder_name,'*.gif')))\n",
    "    # train_y = train_masked_retina_images\n",
    "    \n",
    "\n",
    "    # print (train_x)\n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3aa8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f749155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_argumentation(images, masks, save_path, augment=True):\n",
    "    # size = (512, 512)\n",
    "    target_size = (1024, 1024)\n",
    "\n",
    "    for idx, (x, y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
    "        print(x)\n",
    "        print(y)\n",
    "        # make sure image and mask has same name\n",
    "        # extract the name\n",
    "        name = x.split('\\\\')[-1].split('.')[0]\n",
    "        print(name)\n",
    "\n",
    "        # read the image and the mask\n",
    "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        y = imageio.mimread(y)[0] # mask will not show channel, it is a gray scale image which only has one channel\n",
    "        print(x.shape, y.shape)\n",
    "\n",
    "        if augment == True:\n",
    "            aug = HorizontalFlip(p=1.0) # probability set to 100%\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x1 = augmented['image']\n",
    "            y1 = augmented['mask']\n",
    "            \n",
    "            aug = VerticalFlip(p=1.0) # probability set to 100%\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x2 = augmented['image']\n",
    "            y2 = augmented['mask']\n",
    "\n",
    "            aug = Rotate(limit=45, p=1.0) # probability set to 100%, rotate 45 degree\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x3 = augmented['image']\n",
    "            y3 = augmented['mask']\n",
    "\n",
    "            X = [x, x1, x2, x3]\n",
    "            Y = [y, y1, y2 ,y3]\n",
    "            \n",
    "        else:\n",
    "            X = [x]\n",
    "            Y = [y]\n",
    "\n",
    "        index = 0\n",
    "        for i, m in zip(X, Y):\n",
    "            # # resize\n",
    "            # i = cv2.resize(i, size)\n",
    "            # m = cv2.resize(m, size)\n",
    "\n",
    "            # Padding\n",
    "            i = pad_to_target(i, target_size)\n",
    "            m = pad_to_target(m, target_size, is_mask=True)\n",
    "            \n",
    "            tmp_image_name = f'{name}_{index}.png'\n",
    "            tmp_mask_name = f'{name}_{index}.png'\n",
    "\n",
    "            image_path = os.path.join(save_path, 'image', tmp_image_name)\n",
    "            mask_path = os.path.join(save_path, 'mask', tmp_mask_name)\n",
    "            \n",
    "            cv2.imwrite(image_path, i)\n",
    "            cv2.imwrite(mask_path, m)\n",
    "\n",
    "            index += 1\n",
    "\n",
    "def pad_to_target(img, target_size, is_mask=False):\n",
    "    height, width = img.shape[:2]\n",
    "    # Calculate the padding needed on each side to reach the target size\n",
    "    delta_w = target_size[1] - width\n",
    "    delta_h = target_size[0] - height\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "    \n",
    "    # Use black for padding, which is common in medical imaging\n",
    "    color = 0 if is_mask else [0, 0, 0]  # Black padding\n",
    "    # Apply the calculated padding to each side of the image or mask\n",
    "    padded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "    return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec975695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20 - 20\n",
      "Test: 20 - 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:00<00:03,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_21_30\\21.tif\n",
      "./label_images\\21.tif\n",
      "21\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_21_30\\22.tif\n",
      "./label_images\\22.tif\n",
      "22\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▍                                                                      | 3/20 [00:00<00:02,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_21_30\\23.tif\n",
      "./label_images\\23.tif\n",
      "23\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_21_30\\24.tif\n",
      "./label_images\\24.tif\n",
      "24\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████▊                                                              | 5/20 [00:00<00:01,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_21_30\\25.tif\n",
      "./label_images\\25.tif\n",
      "25\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_21_30\\26.tif\n",
      "./label_images\\26.tif\n",
      "26\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████                                                      | 7/20 [00:00<00:01,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_21_30\\27.tif\n",
      "./label_images\\27.tif\n",
      "27\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_21_30\\28.tif\n",
      "./label_images\\28.tif\n",
      "28\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████▎                                             | 9/20 [00:01<00:01,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_21_30\\29.tif\n",
      "./label_images\\29.tif\n",
      "29\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_21_30\\30.tif\n",
      "./label_images\\30.tif\n",
      "30\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████                                     | 11/20 [00:01<00:00,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_31_40\\31.tif\n",
      "./label_images\\31.tif\n",
      "31\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_31_40\\32.tif\n",
      "./label_images\\32.tif\n",
      "32\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████████████████▎                            | 13/20 [00:01<00:00,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_31_40\\33.tif\n",
      "./label_images\\33.tif\n",
      "33\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_31_40\\34.tif\n",
      "./label_images\\34.tif\n",
      "34\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████▌                    | 15/20 [00:01<00:00,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_31_40\\35.tif\n",
      "./label_images\\35.tif\n",
      "35\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_31_40\\36.tif\n",
      "./label_images\\36.tif\n",
      "36\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████████████████████████████████████████▋            | 17/20 [00:01<00:00,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_31_40\\37.tif\n",
      "./label_images\\37.tif\n",
      "37\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_31_40\\38.tif\n",
      "./label_images\\38.tif\n",
      "38\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████████████████████████▉    | 19/20 [00:02<00:00,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_31_40\\39.tif\n",
      "./label_images\\39.tif\n",
      "39\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_31_40\\40.tif\n",
      "./label_images\\40.tif\n",
      "40\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.90it/s]\n",
      " 15%|████████████▍                                                                      | 3/20 [00:00<00:00, 25.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_01_10\\1.tif\n",
      "./label_images\\1.tif\n",
      "1\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_01_10\\10.tif\n",
      "./label_images\\10.tif\n",
      "10\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_01_10\\2.tif\n",
      "./label_images\\11.tif\n",
      "2\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_01_10\\3.tif\n",
      "./label_images\\12.tif\n",
      "3\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_01_10\\4.tif\n",
      "./label_images\\13.tif\n",
      "4\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████▎                                             | 9/20 [00:00<00:00, 24.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_01_10\\5.tif\n",
      "./label_images\\14.tif\n",
      "5\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_01_10\\6.tif\n",
      "./label_images\\15.tif\n",
      "6\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_01_10\\7.tif\n",
      "./label_images\\16.tif\n",
      "7\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_01_10\\8.tif\n",
      "./label_images\\17.tif\n",
      "8\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_01_10\\9.tif\n",
      "./label_images\\18.tif\n",
      "9\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▏                                | 12/20 [00:00<00:00, 24.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_11_20\\11.tif\n",
      "./label_images\\19.tif\n",
      "11\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_11_20\\12.tif\n",
      "./label_images\\2.tif\n",
      "12\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_11_20\\13.tif\n",
      "./label_images\\20.tif\n",
      "13\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_11_20\\14.tif\n",
      "./label_images\\3.tif\n",
      "14\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_11_20\\15.tif\n",
      "./label_images\\4.tif\n",
      "15\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 18/20 [00:00<00:00, 24.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./retina_images_11_20\\16.tif\n",
      "./label_images\\5.tif\n",
      "16\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_11_20\\17.tif\n",
      "./label_images\\6.tif\n",
      "17\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_11_20\\18.tif\n",
      "./label_images\\7.tif\n",
      "18\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_11_20\\19.tif\n",
      "./label_images\\8.tif\n",
      "19\n",
      "(584, 565, 3) (584, 565)\n",
      "./retina_images_11_20\\20.tif\n",
      "./label_images\\9.tif\n",
      "20\n",
      "(584, 565, 3) (584, 565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 24.19it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Seed\n",
    "    random.seed(0)\n",
    "\n",
    "    # Load data\n",
    "    data_path = './'\n",
    "    test_retina_image_folders = ['retina_images_01_10', 'retina_images_11_20']\n",
    "    train_retina_image_folders = ['retina_images_21_30', 'retina_images_31_40']\n",
    "    masked_retina_image_folder = 'label_images'\n",
    "    # masked_retina_image_folder = 'mask_images'\n",
    "    \n",
    "    (train_x, train_y), (test_x, test_y) = load_data(data_path, test_retina_image_folders, train_retina_image_folders, masked_retina_image_folder)\n",
    "\n",
    "    # Print the length of the data\n",
    "    print(f'Train: {len(train_x)} - {len(train_y)}')\n",
    "    print(f'Test: {len(test_x)} - {len(test_y)}')\n",
    "    # print(test_y)\n",
    "\n",
    "    # Create directory to save image from data argumentation\n",
    "    create_dir('new_data/train/image/')\n",
    "    create_dir('new_data/train/mask/')\n",
    "    create_dir('new_data/test/image/')\n",
    "    create_dir('new_data/test/mask/')\n",
    "\n",
    "    # Data Argumentation\n",
    "    data_argumentation(train_x, train_y, 'new_data/train/', augment=True)\n",
    "    data_argumentation(test_x, test_y, 'new_data/test/', augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "352d0ecf-d861-4a8e-9d68-cc96716e48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    # in_c : number of input channel\n",
    "    # out_c : number of output channel\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3 x 3 convolution layer\n",
    "        # Padding in CNN refers to the addition of extra pixels around the borders of the input images or feature map\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        \n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # print(x.shape)\n",
    "    \n",
    "        # input of second layer is output from first layar\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Pooling Layers, also known as downsample layers, are an essential component of convolutional neural networks \n",
    "        (CNNs) used in deep learning. \n",
    "        They are responsible for reducing the spatial dimensions of the input data, in terms of width and height, \n",
    "        while retaining the most important information.\n",
    "        \"\"\"\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2)) # 2 x 2 max pooling\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        \"\"\"\n",
    "        Example:\n",
    "        torch.Size([2, 64, 128, 128])\n",
    "        torch.Size([2, 64, 128, 128]) torch.Size([2, 64, 64, 64])\n",
    "        \"\"\"\n",
    "        return x, p # x is output of convolution block and p is output of pooling block\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        # 2 x 2 transpose convolution, we want to increase / upsample so:\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c + out_c, out_c)\n",
    "        \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1) # axis one contain the number of channel\n",
    "        x = self.conv(x) \n",
    "\n",
    "        return(x)\n",
    "        \n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ## Encoder ##\n",
    "        self.e1 = encoder_block(3, 64)     # 3 because rgb got 3 channel\n",
    "        self.e2 = encoder_block(64, 128)   # 3 because rgb got 3 channel\n",
    "        self.e3 = encoder_block(128, 256)  # 3 because rgb got 3 channel\n",
    "        self.e4 = encoder_block(256, 512)  # 3 because rgb got 3 channel\n",
    "        self.e5 = encoder_block(512, 1024) # 3 because rgb got 3 channel\n",
    "\n",
    "        ## Bottleneck ##\n",
    "        self.b = conv_block(1024, 2048) # Bridge layer with just conv_block\n",
    "\n",
    "        ## Decoder ##\n",
    "        self.d1 = decoder_block(2048, 1024) # the feature highend increases by 2 output feature map decreasre by 2\n",
    "        self.d2 = decoder_block(1024, 512) # the feature highend increases by 2 output feature map decreasre by 2\n",
    "        self.d3 = decoder_block(512, 256)  # the feature highend increases by 2 output feature map decreasre by 2\n",
    "        self.d4 = decoder_block(256, 128)  # the feature highend increases by 2 output feature map decreasre by 2\n",
    "        self.d5 = decoder_block(128, 64)   # the feature highend increases by 2 output feature map decreasre by 2\n",
    "        \n",
    "        ## Classifier ##\n",
    "        # To generate the segmentation map\n",
    "        # 1 because mask is one channel color only\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        ## Encoder ##\n",
    "        # Act as skip connection for the decoder\n",
    "        s1, p1 = self.e1(inputs) # s1 skip connection p1 pooling output\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "        s5, p5 = self.e5(p4)\n",
    "\n",
    "        ## Bottleneck ## \n",
    "        b = self.b(p5)\n",
    "        # print(s1.shape, s2.shape, s3.shape, s4.shape)\n",
    "        \"\"\"\n",
    "        torch.Size([2, 64, 512, 512])  - 512 x 512 same as input size \n",
    "        torch.Size([2, 128, 256, 256]) - half of input size\n",
    "        torch.Size([2, 256, 128, 128]) \n",
    "        torch.Size([2, 512, 64, 64])\n",
    "        torch.Size([2, 1024, 32, 32])  - at bridge is 32\n",
    "        \"\"\"\n",
    "        # print(b.shape)\n",
    "\n",
    "        ## Decoder ##\n",
    "        d1 = self.d1(b, s5)\n",
    "        d2 = self.d2(d1, s4)\n",
    "        d3 = self.d3(d2, s3)\n",
    "        d4 = self.d4(d3, s2)\n",
    "        d5 = self.d5(d4, s1)\n",
    "\n",
    "        outputs = self.outputs(d5)\n",
    "\n",
    "        # print(f'd4.shape:{d4.shape}')\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # define batch size, number of channel, batch, width\n",
    "    x = torch.randn((2, 3, 1024, 1024))\n",
    "    f = build_unet()\n",
    "    y = f(x)\n",
    "    # print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5b4a8a8-6f48-4085-8407-7c3dce60763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "\n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e6b40d-635b-4227-9f47-f0242043b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Calculate the time taken \"\"\"\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c59f67e9-f25d-403d-b176-b2ee978e386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path):\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.n_samples = len(images_path)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ## Read image ##\n",
    "        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)\n",
    "        image = image / 255.0 # Normalize the image , cv2 image is channel last, (512, 512 ,3)\n",
    "        image = np.transpose(image, (2, 0, 1)) # pytorch is channel first approach, (3, 512, 512)\n",
    "        image = image.astype(np.float32)\n",
    "        image = torch.from_numpy(image) # create a tensor from a NumPy array\n",
    "\n",
    "        ## Read mask ##\n",
    "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = mask / 255.0 # (512, 512)\n",
    "        mask = np.expand_dims(mask, axis = 0) # np.expand_dims will expand the dimension by 1, axis = 0 is expand the dimension at the front, expand to first axis, (1, 512, 512)\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask = torch.from_numpy(mask)\n",
    "\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of sample\n",
    "        \"\"\"\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ed486-d72e-4133-90ae-394cf1152cad",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3391eff-d081-4b7b-a6b6-cbf5dd5f5027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in loader: # x and y are tensors\n",
    "        x = x.to(device, dtype=torch.float32)\n",
    "        y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y) # y is ground truth value\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss / len(loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader: # x and y are tensors\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.float32)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss / len(loader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ff6bf-f62b-4970-8375-40bde3df5411",
   "metadata": {},
   "source": [
    "Executing the training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b98de645-e621-4a2d-967e-fbcc512c5381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "Train: 80 - Valid: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/50 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.71 GiB is allocated by PyTorch, and 686.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[0;32m     58\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 60\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, train_loader, optimizer, loss_fn, device)\n\u001b[0;32m     61\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, loss_fn, device)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m## Save the model ##\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[0;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y) \u001b[38;5;66;03m# y is ground truth value\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iip2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iip2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 122\u001b[0m, in \u001b[0;36mbuild_unet.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    120\u001b[0m d3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md3(d2, s3)\n\u001b[0;32m    121\u001b[0m d4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md4(d3, s2)\n\u001b[1;32m--> 122\u001b[0m d5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md5(d4, s1)\n\u001b[0;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs(d5)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# print(f'd4.shape:{d4.shape}')\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iip2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\iip2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 65\u001b[0m, in \u001b[0;36mdecoder_block.forward\u001b[1;34m(self, inputs, skip)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, skip):\n\u001b[0;32m     64\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(inputs)\n\u001b[1;32m---> 65\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, skip], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# axis one contain the number of channel\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x) \n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(x)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.71 GiB is allocated by PyTorch, and 686.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    random.seed(0)\n",
    "\n",
    "    ## Directory ##\n",
    "    create_dir('files')\n",
    "\n",
    "    ## Load dataset ##\n",
    "    train_x = sorted(glob('./new_data/train/image/*'))\n",
    "    train_y = sorted(glob('./new_data/train/mask/*'))\n",
    "    \n",
    "    valid_x = sorted(glob('./new_data/test/image/*'))\n",
    "    valid_y = sorted(glob('./new_data/test/mask/*'))\n",
    "\n",
    "    data_str = f'Dataset Size:\\nTrain: {len(train_x)} - Valid: {len(valid_x)}'\n",
    "    print(data_str)\n",
    "\n",
    "    ## Hyperparameters ##\n",
    "    # H = 512\n",
    "    # W = 512\n",
    "    H = 1024\n",
    "    W = 1024\n",
    "    size = (H, W)\n",
    "    batch_size = 2\n",
    "    num_epochs = 50\n",
    "    lr = 1e-4 # learning rate\n",
    "    checkpoint_path = 'files/checkpoint.pth'\n",
    "\n",
    "    ## Dataset and Loader ##\n",
    "    train_dataset = RetinaDataset(train_x, train_y)\n",
    "    valid_dataset = RetinaDataset(valid_x, valid_y)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset = train_dataset,\n",
    "        batch_size = batch_size, # sample per batch to load\n",
    "        shuffle = True,\n",
    "        num_workers = 0 # num of subprocess\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset = valid_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False,\n",
    "        num_workers = 0\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda') # GTX2060 6GB\n",
    "    model = build_unet()\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "    loss_fn = DiceBCELoss() # Dice + BCE Loss\n",
    "\n",
    "    ## Training the model ##\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "        valid_loss = evaluate(model, valid_loader, loss_fn, device)\n",
    "\n",
    "        ## Save the model ##\n",
    "        if valid_loss < best_valid_loss:\n",
    "            data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
    "            print(data_str)\n",
    "\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)            \n",
    "            \n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
    "        data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
    "        data_str += f'\\t Val. Loss: {valid_loss:.3f}\\n'\n",
    "        print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "567101e3-37c2-42bf-ba81-51db1f1ccc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  8.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def mask_parse(mask):\n",
    "    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)\n",
    "    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)\n",
    "    return mask\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ## Seeding ##\n",
    "    random.seed(0)\n",
    "\n",
    "    ## Create folder ##\n",
    "    create_dir('results')\n",
    "    \n",
    "    ## load dataset ##\n",
    "    test_x = sorted(glob(\"./new_data/test/image/*\"))\n",
    "    test_y = sorted(glob(\"./new_data/test/mask/*\"))\n",
    "\n",
    "    # print(test_x)\n",
    "        \n",
    "    ## Hyperparameters ##\n",
    "    # W = 512\n",
    "    # H = 512\n",
    "    W = 1024\n",
    "    H = 1024\n",
    "    size = (W, H)\n",
    "    checkpoint_path = 'files/checkpoint.pth'\n",
    "    \n",
    "    ## Load checkpoint ##\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = build_unet()\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    time_taken = []\n",
    "        \n",
    "    for i, x in tqdm(enumerate(test_x), total=len(test_x)):\n",
    "        ## Extract the name ##\n",
    "        name = x.split('\\\\')[-1].split('.')[0]\n",
    "        # print(name)\n",
    "        \n",
    "        ## Reading image ##\n",
    "        image = cv2.imread(x, cv2.IMREAD_COLOR) # (512, 512, 3)\n",
    "        x = np.transpose(image, (2, 0, 1)) # (3, 512, 512)\n",
    "        x = x / 255.0\n",
    "        x = np.expand_dims(x, axis=0) # (1, 3, 512, 512)\n",
    "        x = x.astype(np.float32)\n",
    "        x = torch.from_numpy(x)\n",
    "        x = x.to(device)\n",
    "\n",
    "        with torch.no_grad(): # disables gradient calculation (We dont want this during inference)\n",
    "            ## Inference ##\n",
    "            start_time = time.time()\n",
    "            pred_y = model(x)\n",
    "            pred_y = torch.sigmoid(pred_y)\n",
    "            total_time = time.time() - start_time\n",
    "            time_taken.append(total_time)\n",
    "\n",
    "            pred_y = pred_y[0].cpu().numpy() # (1, 512, 512)\n",
    "            pred_y = np.squeeze(pred_y, axis=0) # (512, 512)\n",
    "\n",
    "            # Thresholding the predicted mask\n",
    "            pred_y = pred_y > 0.5\n",
    "            pred_y = np.array(pred_y, dtype=np.uint8) # data type unsigned int\n",
    "\n",
    "        ## Save mask ##\n",
    "        pred_y = mask_parse(pred_y)\n",
    "\n",
    "        # Upscale the mask to 565x584\n",
    "        pred_y_resized = cv2.resize(pred_y, (565, 584), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "        # Save mask in TIFF format\n",
    "        cv2.imwrite(f'results/{name}.tif', pred_y_resized * 255)\n",
    "        # cv2.imwrite(f'results/{name}.png', pred_y * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6952d620-65c7-4051-aeec-ba0b835e7636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "P Average:  0.7751080475420754\n",
      "N Average:  0.9638021258694751\n",
      "T Average:  0.9393800747145725\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "\n",
    "# img_path_1 = r'C:\\Users\\11504\\Desktop\\iip\\cw\\retina_images_01_10'\n",
    "# img_path_2 = r'C:\\Users\\11504\\Desktop\\iip\\cw\\retina_images_11_20'\n",
    "# mask_path = r'C:\\Users\\11504\\Desktop\\iip\\cw\\mask_images'\n",
    "# label_path = r'C:\\Users\\11504\\Desktop\\iip\\cw\\label_images'\n",
    "img_path_1 = r'.\\retina_images_01_10'\n",
    "img_path_2 = r'.\\retina_images_11_20'\n",
    "mask_path = r'.\\mask_images'\n",
    "label_path = r'.\\label_images'\n",
    "results_path = './results'\n",
    "\n",
    "P_total, N_total, T_total = 0., 0., 0.\n",
    "\n",
    "for i in range(1, 21):\n",
    "    # Load input image\n",
    "    # if i < 11:\n",
    "    #     img = cv2.imread(os.path.join(img_path_1,str(i)+'.tif'),cv2.IMREAD_COLOR)\n",
    "    # else:\n",
    "    #     img = cv2.imread(os.path.join(img_path_2,str(i)+'.tif'),cv2.IMREAD_COLOR)\n",
    "        \n",
    "    # # Segment the retina blood vessel\n",
    "    # imgGreen = img[:,:,1]\n",
    "    # blur1 = cv2.GaussianBlur(imgGreen,(9,9),0)\n",
    "    # blur2 = cv2.GaussianBlur(imgGreen,(11,11),0)\n",
    "    # DoG = blur1-blur2;\n",
    "\n",
    "    # Load and Preprocess the mask\n",
    "    mask = np.array(PIL.Image.open(os.path.join(mask_path,str(i)+'.tif')))\n",
    "    # mask_DoG = np.multiply(DoG,mask/255)\n",
    "    # thres_DoG = mask_DoG>220\n",
    "    # unique_values = np.unique(mask)\n",
    "    # print(\"Unique values in the mask:\", unique_values)\n",
    "    \n",
    "    thres_DoG = cv2.imread(os.path.join(results_path, str(i) + '_0.tif'), cv2.IMREAD_GRAYSCALE)\n",
    "    thres_DoG = (thres_DoG == 255).astype(np.uint8)\n",
    "    print(np.unique(thres_DoG))\n",
    "    \n",
    "    # Load the label\n",
    "    label = np.array(PIL.Image.open(os.path.join(label_path,str(i)+'.tif')))\n",
    "    \n",
    "    TP = sum(sum((label==255) & (thres_DoG==1) & (mask == 255)))\n",
    "    TN = sum(sum((label==0) & (thres_DoG==0) & (mask == 255)))\n",
    "    FP = sum(sum((label==0) & (thres_DoG==1) & (mask == 255)))\n",
    "    FN = sum(sum((label==255) & (thres_DoG==0) & (mask == 255)))\n",
    "    P_total += TP/(TP+FN)\n",
    "    N_total += TN/(TN+FP)\n",
    "    T_total += (TP+TN)/(TP+FN+TN+FP)\n",
    "\n",
    "print('P Average: ', P_total/20)\n",
    "print('N Average: ', N_total/20)\n",
    "print('T Average: ', T_total/20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
